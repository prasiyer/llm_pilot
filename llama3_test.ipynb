{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9888aef6b043b1b2e999c6e91dbd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "llama3_hf_token = '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'\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token)\n",
    "# create pipeline with cuda\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c521dc1a674f16bbc0375fac9e4dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", load_in_4bit = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\") ## pip install accelerate based on error message\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# question_to_model = \"Who is the CEO of Tesla\"\n",
    "# model_inputs = tokenizer([question_to_model], return_tensors=\"pt\").to(\"cuda\")\n",
    "# model_inputs = tokenizer([question_to_model], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipe(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "# print the generated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "Yes, we do! Based on your taste in \"Breaking Bad\" and \"Band of Brothers\", here are some recommendations that you might enjoy:\n",
      "\n",
      "1. \"Narcos\" - This Netflix series is a true-crime drama that follows the rise and fall of Colombian drug lord Pablo Escobar and the DEA agents who are tasked with taking him down.\n",
      "2. \"The Wire\" - This critically acclaimed HBO series is a gritty and realistic portrayal of the drug war in Baltimore, following a group of detectives as they try to bring down a drug kingpin.\n",
      "3. \"The Sopranos\" - Another classic HBO series, \"The Sopranos\" is a crime drama that follows the life of a New Jersey mob boss and his family.\n",
      "4. \"Oz\" - This HBO series is a gritty and intense portrayal of life inside a maximum-security prison,\n"
     ]
    }
   ],
   "source": [
    "for sequence in sequences:\n",
    "    print(sequence['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the CEO of Tesla, Inc.?\n",
      "A) Elon Musk\n",
      "B) Jeff Bezos\n",
      "C) Mark Zuckerberg\n",
      "D) Sergey Brin\n",
      "\n",
      "Answer: A) Elon Musk\n",
      "\n",
      "Explanation: Elon Musk is the CEO and CTO of Tesla, Inc., a company that specializes in electric vehicles, clean energy solutions, and other innovative technologies. He has been the CEO of Tesla since 2008 and has played a key role in the company's growth and success. Musk is also the\n"
     ]
    }
   ],
   "source": [
    "question_to_model = \"Who is the CEO of Tesla\"\n",
    "# prompt the llama 3 model with the question and print the answer\n",
    "output = pipe(question_to_model, max_length=100)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \"Who is the CEO of Tesla, Inc.?\\nA) Elon Musk\\nB) Jeff Bezos\\nC) Mark Zuckerberg\\nD) Sergey Brin\\n\\nAnswer: A) Elon Musk\\n\\nExplanation: Elon Musk is the CEO and CTO of Tesla, Inc., a company that specializes in electric vehicles, clean energy solutions, and other innovative technologies. He has been the CEO of Tesla since 2008 and has played a key role in the company's growth and success. Musk is also the\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrrr, shiver me timbers! Ye be askin' a mighty strange question, matey! Helicopters be big, noisy contraptions made o' metal and wires, not a tasty snack fit fer a swashbucklin' pirate like yerself!\\n\\nNow, I be thinkin' ye be pullin' me leg, but just fer kicks, let's have a bit o' fun with this question. If we assume a helicopter be made o' edible materials (which be a mighty big assumption, matey!), I reckon a human could maybe, just maybe, eat a small toy helicopter or\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Elon Musk is the CEO of Tesla, Inc.'}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a professional tone and answers in 1 sentence\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Who is the CEO of TESLA?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=20)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"It's gotta be Goku, because who wouldn't want to be a powerful, spiky-haired,\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a funny way and in 1 sentence\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Who is the most popular dragonball character?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=20)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a friendly chatbot who always responds in a funny way and in 1 sentence<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho is the most popular dragonball character?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIt's gotta be Goku, because who wouldn't want to be a powerful, spiky-haired, ki-blasting superhero who can fly and turn into a giant monkey?<|eot_id|>\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_tokens = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "# generate output\n",
    "outputs = model.generate(message_tokens, max_new_tokens=128) \n",
    "# convert ids to text\n",
    "tokenizer.decode(outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Elon Musk was born on June 28, 1971, which makes him 51 years'}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a professional tone and answers in 1 sentence\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Who is the CEO of TESLA?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Elon Musk is the CEO of Tesla, Inc.?\"},\n",
    "    {\"role\": \"user\", \"content\": \"How old is he?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=20)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Virat Kohli lives in a cricket-filled bubble, but if I had to guess, I'd say he's probably in Bangalore, India, where the coffee is strong and the wickets are weaker!\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a funny way and in 1 sentence\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Where does Virat Kohli live?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=100)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"According to various polls, Prime Minister Narendra Modi's approval rating in India has consistently ranged between 50% to 70% since 2014, with some surveys showing a slight decline in recent years.\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a factual way and in 1 sentence\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is Prime Minister Modi's approval rating?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=100)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
