{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "llama3_hf_token = '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'\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token)\n",
    "# create pipeline with cuda\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", load_in_4bit = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\") ## pip install accelerate based on error message\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# question_to_model = \"Who is the CEO of Tesla\"\n",
    "# model_inputs = tokenizer([question_to_model], return_tensors=\"pt\").to(\"cuda\")\n",
    "# model_inputs = tokenizer([question_to_model], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_to_model = \"Who is the CEO of Tesla\"\n",
    "model_inputs = tokenizer([question_to_model], return_tensors=\"pt\").to(\"cuda\")\n",
    "# calculate the number of tokens in model_inputs\n",
    "num_tokens = model_inputs['input_ids'].shape[1]\n",
    "# print the number of tokens\n",
    "print(num_tokens)\n",
    "# generate output using the model\n",
    "output = model.generate(**model_inputs, max_length=num_tokens+50, num_return_sequences=5)\n",
    "# decode the output\n",
    "output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "# print the output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipe(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "# print the generated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in sequences:\n",
    "    print(sequence['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_to_model = \"Who is the CEO of Tesla\"\n",
    "# prompt the llama 3 model with the question and print the answer\n",
    "output = pipe(question_to_model, max_length=100)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_tokens = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "# generate output\n",
    "outputs = model.generate(message_tokens, max_new_tokens=128) \n",
    "# convert ids to text\n",
    "tokenizer.decode(outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a factual manner and answer in a single sentence\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is cluster analysis?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=20)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a factual manner and answer in a single sentence. In addition to your knowledge, you will also use the following information on hydraulic hose types ---\\\n",
    "            Class 1:Same as 30R2, Type 1, per SAE J30 (latest issue). Reinforced between tube and cover with one ply of braided, knit, spiral or woven fabric. \\\n",
    "                Class 2:Same as 30R2, Type 2, per SAE J30 (latest issue). Reinforced between tube and cover with two braided plies of woven fabric. \\\n",
    "                    Class 3:Same as 30R2, Type 3, per SAE J30 (latest issue). Reinforced between tube and cover with one braided ply of textile yarn. \\\n",
    "                        Class 4:Same as 100R4, per SAE J517 (latest issue). Usually used for vacuum application.Reinforced between tube and cover with a ply or plies of woven or braided textile\"\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is cluster analysis?\"},\n",
    "    {'role': 'assistant', 'content': 'Cluster analysis is a type of unsupervised machine learning technique used to group similar objects or data points'},\n",
    "    {\"role\": \"user\", \"content\": \"What are its benefits?\"},\n",
    "    {'role': 'assistant', 'content': 'Cluster analysis helps to identify patterns, relationships, and structures in data, enables data visualization, and facilitates'},\n",
    "    {\"role\": \"user\", \"content\": \"What is class 1?\"},\n",
    "\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=20)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate all the content from messages\n",
    "content = \"\".join([message[\"content\"] for message in messages])\n",
    "# tokenize the content\n",
    "tokens = tokenizer(content, return_tensors=\"pt\").to(\"cuda\")\n",
    "# get the number of tokens\n",
    "num_tokens = tokens['input_ids'].shape[1]\n",
    "# print the number of tokens\n",
    "print(num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
