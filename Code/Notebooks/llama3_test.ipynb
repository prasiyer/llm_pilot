{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTEBOOK FOR TESTING LLAMA-3-8B-INSTRUCT ###\n",
    "#### TESTING 2 APPROACHES FOR LOADING MODEL ####\n",
    "#### 1) Using pipeline\n",
    "#### 2) Loading model using AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 2\n",
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", load_in_4bit = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\") ## pip install accelerate based on error message\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# question_to_model = \"Who is the CEO of Tesla\"\n",
    "# model_inputs = tokenizer([question_to_model], return_tensors=\"pt\").to(\"cuda\")\n",
    "# model_inputs = tokenizer([question_to_model], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt the model and print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_to_model = \"Who is the CEO of Tesla\"\n",
    "model_inputs = tokenizer([question_to_model], return_tensors=\"pt\").to(\"cuda\")\n",
    "# calculate the number of tokens in model_inputs\n",
    "num_tokens = model_inputs['input_ids'].shape[1]\n",
    "# print the number of tokens\n",
    "print(num_tokens)\n",
    "# generate output using the model\n",
    "output = model.generate(**model_inputs, max_length=num_tokens+50, num_return_sequences=5)\n",
    "# decode the output\n",
    "output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "# print the output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 1\n",
    "#### Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27092615aee04795af6e0849b334f574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "llama3_hf_token = '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'\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token)\n",
    "# create pipeline with cuda\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token, device=0)\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token, device=0, model_kwargs={\n",
    "  #      \"torch_dtype\": torch.float16,\n",
    "   #     \"quantization_config\": {\"load_in_4bit\": True}},)\n",
    "# without device\n",
    "#pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token,device_map = \"cuda:0\" , model_kwargs={\n",
    " #       \"torch_dtype\": torch.float16,\n",
    "  #      \"quantization_config\": {\"load_in_4bit\": True}},)\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token,device_map = \"auto\" , model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        # \"quantization_config\": {\"load_in_4bit\": True}},\n",
    "        \"quantization_config\": {\"load_in_4bit\": True,\"bnb_4bit_compute_dtype\":torch.float16}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMPT THE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly send the prompt as a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipe(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "# print the generated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in sequences:\n",
    "    print(sequence['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_to_model = \"Who is the CEO of Tesla\"\n",
    "# prompt the llama 3 model with the question and print the answer\n",
    "output = pipe(question_to_model, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send the prompt using message template to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "output = pipe(messages, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a factual manner and answer in a single sentence\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is cluster analysis?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=20)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a factual manner and answer in a single sentence\",\n",
    "    },\n",
    "    \n",
    "]\n",
    "print(pipe(messages, max_new_tokens=20)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a factual manner and answer in a single sentence. In addition to your knowledge, you will also use the following information on hydraulic hose types ---\\\n",
    "            Class 1:Same as 30R2, Type 1, per SAE J30 (latest issue). Reinforced between tube and cover with one ply of braided, knit, spiral or woven fabric. \\\n",
    "                Class 2:Same as 30R2, Type 2, per SAE J30 (latest issue). Reinforced between tube and cover with two braided plies of woven fabric. \\\n",
    "                    Class 3:Same as 30R2, Type 3, per SAE J30 (latest issue). Reinforced between tube and cover with one braided ply of textile yarn. \\\n",
    "                        Class 4:Same as 100R4, per SAE J517 (latest issue). Usually used for vacuum application.Reinforced between tube and cover with a ply or plies of woven or braided textile\"\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is cluster analysis?\"},\n",
    "    {'role': 'assistant', 'content': 'Cluster analysis is a type of unsupervised machine learning technique used to group similar objects or data points'},\n",
    "    {\"role\": \"user\", \"content\": \"What are its benefits?\"},\n",
    "    {'role': 'assistant', 'content': 'Cluster analysis helps to identify patterns, relationships, and structures in data, enables data visualization, and facilitates'},\n",
    "    {\"role\": \"user\", \"content\": \"What is class 1?\"},\n",
    "\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=20)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_file = \"/home/vp899/projects/llm_pilot/Data/MAT1130_Context.txt\"\n",
    "text_file = \"/home/vp899/projects/llm_pilot/Data/MAT1130_Context - shorter.txt\"\n",
    "# read the contents of text_file into str\n",
    "with open(text_file, 'r') as file:\n",
    "    input_text = file.read()\n",
    "# print(text)\n",
    "system_prompt = \"You are a helpful digital assistant.\\\n",
    "    You will provide clear answers in 3 sentences. Your main reference is the text included within triple quotes.At the beginning of each answer,\\\n",
    "        you will include the main json tag from the portion of the reference text that you are using for the answer.''' \" +  input_text + \" '''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What are the key points in MAT1130?\"},\n",
    "    \n",
    "]\n",
    "output = pipe(messages, max_new_tokens=500)[0]['generated_text'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '**\"TITLE\"**: Material Specification for HYDRAULIC LINE TUBING - MAT1130**\\n\\nThe key points in MAT1130 are:\\n\\n1. The specification provides reference information for CNH hydraulic line tubing grades listed in Table 1.\\n2. It is intended to replace Former CNH Company Material Specifications listed in Table 4 and should be used on all applicable new and updated engineering drawings.\\n3. All National Standards and related test method designations are to be latest issue unless otherwise specified.\\n\\nNote: This summary is based on the main json tag \"TITLE\" and provides a brief overview of the key points in MAT1130.'}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is MAT1130?\"},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '**\"TITLE\"**: Material Specification for HYDRAULIC LINE TUBING - MAT1130**\\n\\nThe key points in MAT1130 are:\\n\\n* Provides reference information for CNH hydraulic line tubing grades listed in Table 1.\\n* Intended to replace Former CNH Company Material Specifications listed in Table 4.\\n* Applicable to steel hydraulic line tubing used for applications where fluids are transferred under pressure.\\n* Grades A, B, BF, C, E, and F are furnished normalized or annealed.\\n* Grade A tubing can be used for applications specifying Grade B or BF tubing, and vice versa.\\n* Grade C tubing can be used for applications specifying Grade A tubing, but requires review and approval by CNH Industrial Design Engineering and Manufacturing Engineering.\\n* Dimensional differences are particularly important for brazed applications since clearances between tubing OD and fitting ID significantly affect brazed joint integrity.'}\n"
     ]
    }
   ],
   "source": [
    "print(pipe(messages, max_new_tokens=500)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442\n"
     ]
    }
   ],
   "source": [
    "content = \"\".join([message[\"content\"] for message in messages])\n",
    "tokens = tokenizer(content, return_tensors=\"pt\").to(\"cuda\")\n",
    "num_tokens = tokens['input_ids'].shape[1]\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate all the content from messages\n",
    "content = \"\".join([message[\"content\"] for message in messages])\n",
    "# tokenize the content\n",
    "tokens = tokenizer(content, return_tensors=\"pt\").to(\"cuda\")\n",
    "# get the number of tokens\n",
    "num_tokens = tokens['input_ids'].shape[1]\n",
    "# print the number of tokens\n",
    "print(num_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROACH 3\n",
    "#### AutoModelForCausalLM && load_in_4bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True,\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_quant_type=\"nf4\",\n",
    "bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_4b = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"/home/vp899/projects/llm_pilot/Data/MAT1130_Context.txt\"\n",
    "# read the contents of text_file into str\n",
    "with open(text_file, 'r') as file:\n",
    "    input_text = file.read()\n",
    "# print(text)\n",
    "system_prompt = \"You are a helpful digital assistant.\\\n",
    "    You will provide clear answers in 3 sentences. Your main reference is the text included within triple quotes.At the beginning of each answer,\\\n",
    "        you will include the main json tag from the portion of the reference text that you are using for the answer.''' \" +  input_text + \" '''\"\n",
    "system_prompt = \"You are a helpful digital assistant.\\\n",
    "    You will provide clear answers in 3 sentences. Your main reference is the text included within triple quotes.''' \" +  input_text + \" '''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_token = tokenizer(system_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_token.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What are the key points in MAT1130?\"},\n",
    "    \n",
    "]\n",
    "# output = pipe(messages, max_new_tokens=500)[0]['generated_text'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\") --- this is not working\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "# model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output using the model\n",
    "output = model_4b.generate(model_inputs, max_new_tokens = 500, eos_token_id = terminators, do_sample=True, temperature=0.8, top_p =0.9)\n",
    "# output = model_4b.generate(model_inputs.input_ids, max_new_tokens = 500, eos_token_id = terminators, do_sample=True, temperature=0.8, top_p =0.9)\n",
    "# decode the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output to a text file\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARIZE INPUT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbca2f1536814f178efc658e6703521d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\") ## pip install accelerate based on error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"/home/vp899/projects/llm_pilot/Data/MAT1130_Context.txt\"\n",
    "# read the contents of text_file into str\n",
    "with open(text_file, 'r') as file:\n",
    "    input_text = file.read()\n",
    "# print(text)\n",
    "system_prompt = \"You are a helpful digital assistant. You will provide clear and factual answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496\n"
     ]
    }
   ],
   "source": [
    "# find the number of words in the input_text\n",
    "num_words = len(input_text.split())\n",
    "# print the number of words\n",
    "print(num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Can you please create a 1200 word summary of the text within the triple quotes?. Include all the quantitative details ''' \" + input_text + \"'''\"},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model_output = model.generate(model_inputs, max_length = 5500, pad_token_id=tokenizer.eos_token_id, do_sample=True, temperature=0.8, top_p =0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the output\n",
    "output = tokenizer.batch_decode(model_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You are a helpful digital assistant. You will provide clear and factual answersuser\n",
      "\n",
      "Can you please create a 1200 word summary of the text within the triple quotes?. Include all the quantitative details ''' {\"TITLE\": \"Material Specification for HYDRAULIC LINE TUBING - MAT1130\",\n",
      "\"Document\": [\n",
      "{\"SCOPE\": \"1.1) This specification provides reference information for CNH hydraulic line tubing grades listed in Table 1. It is intended to replace Former CNH Company Material Specifications listed in Table 4 and should be used on all applicable new and updated engineering drawings.1.2) All National Standards and related test method designations are to be latest issue unless otherwise specified.\"},\n",
      "{\"CNH Material Grade and Material Description\":\"\n",
      "Following are the CNH Material Grade and Material Description shown within paranthesis:\n",
      "(CNH Grade A: Cold Drawn, Single Wall, Welded Tubing, with No Internal Flash. For use in applications involving transmission of fluid at higher pressure; recommended where high rates of vibration or surge loading are experienced. Available in larger sizes and heavier walls than found in automotive tubing.\n",
      "CNH Grade B: Single Wall, Welded Tubing with Flash Control (0.25 mm / 0.010\" max. flash height). Not recommended for applications involving high rates of vibration or surge loading unless testing verified acceptability.\n",
      "CNH Grade BF: Single Wall Welded Tubing with Flash in. Not recommended for applications involving high rates of vibration or surge loading unless testing verified acceptability. This tubing is not easily formed and requires a larger bend radius than Grades A or B.\n",
      "CNH Grade C: Cold Drawn, Single Wall Seamless Tubing. Same uses as those shown for Grade A. Dimensional tolerances are generally more liberal than those of Grades A or B.\n",
      "CNH Grade D:Brazed Double Wall Tubing (commercially known as Bundy Tube). Used for high pressure, critical applications such as brake and hydraulic service lines.\n",
      "CNH Grade E:Cold Drawn, Single Wall Seamless Tubing (EN 10305-4, E355), or Welded Tubing with no Internal Flash(SAE J2614). Made from high strength low alloy steel. Used in high pressure systems such as 40 MPa (400 bar, 5800 psi) excavator hydraulic systems. In North America, welded SAE J2614 tubing is the most comparable material to seamless EN 10305-4, Grade E355 tubing. Where Grade E seamless tube is specified, use of SAE J2614 shall first be reviewed and approved by CNH Industrial Design Engineering.\n",
      "CNH Grade F:Cold Drawn Single Wall Welded Tubing, Normalized Intended to service higher pressure applications than equivalent sizes of SAE J525 (CNH Grade A).\n",
      "Due to higher carbon content, forming characteristics are diminished versus Grade A. Special attention to the overall forming characteristic of the finished assembly shall be considered when specifying Grade F.)\n",
      "\n",
      "Additional notes on Grade & Description: (1) See ENS0160 for tables and formulae to determine reference working pressures for listed materials.(2) Grades A through D are made from Low Carbon Steel\"},\n",
      "\n",
      "{\"2. APPLICATION\":\"\n",
      "2.1 Steel hydraulic line tubing is used for applications where fluids are transferred under pressure, such as hydraulic, brake, & fuel supply lines. These tubes are produced to chemical and mechanical requirements. Grades A, B, BF (except as-welded SAE J526), C, E, and F are furnished normalized or annealed. Tubing Grades B and BF are the least expensive; however, Grades A & C are most commonly used in North America & Europe, respectively.\n",
      "\n",
      "2.2 It is permissible to use Grade A tubing for applications where Grade B or BF tubing is specified and to use Grade B where Grade BF tubing is specified. Because of differences in dimensional tolerance and/or tube end formability, all other substitutions (for example, using Grade C for applications specifying Grade A) must be reviewed and approved by controlling CNH Industrial Design Engineering and CNH Industrial Manufacturing Engineering prior to proceeding. Dimensional differences are particularly important for brazed applications since clearances between tubing OD and fitting ID significantly affect brazed joint integrity.\"},\n",
      "\n",
      "{\n",
      "  \"Table\": \"Table 2: Local Material and National Standard References\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"CNH Grade\": \"A\",\n",
      "      \"SAE\": \"SAE J525\",\n",
      "      \"DIN\": \"\",\n",
      "      \"EN/ISO\": \"EN 10305-6, E235\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"B\",\n",
      "      \"SAE\": \"SAE J356\": \"\",\n",
      "      \"DIN\": \"\",\n",
      "      \"EN/ISO\": \"EN 10305-3, E235+N Option 3\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"BF\",\n",
      "      \"SAE\":\"SAE J526\",\n",
      "      \"DIN\": \"\",\n",
      "      \"EN/ISO\": \"EN 10305-3, E235+N\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"C\",\n",
      "      \"SAE\":\"SAE J524\",\n",
      "      \"DIN\": \"\",\n",
      "      \"EN/ISO\": \"EN 10305-4, E235\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"D\",\n",
      "      \"SAE\":\"SAE J527\",\n",
      "      \"\"DIN\": \"DIN 74234, Type B\",\n",
      "      \"EN/ISO\": \"ISO 4038\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"E\",\n",
      "      \"SAE\": \"SAE J2614 (1)\",\n",
      "      \"DIN\": \"\",\n",
      "      \"EN/ISO\": \"EN 10305-4, E355\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"F\",\n",
      "      \"SAE\": SAE J2467\": \"\",\n",
      "      \"DIN\": \"\",\n",
      "      \"EN/ISO\": \"EN 10305-6, E275\"\n",
      "    }\n",
      "{\"Note 1\": \"(1) In North America, welded SAE J2614 tubing is the most comparable material to seamless EN 10305-4, Grade E355 tubing. Where Grade E seamless tube is specified, use of SAE J2614 shall first be reviewed and approved by CNH Industrial Design Engineering\"}\n",
      "  ]\n",
      "},\n",
      "{\n",
      "  \"Table\": \"TABLE 3: Tube End Connector Application Guide\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"CNH Grade\": \"A\",\n",
      "      \"Braze\": \"Yes\",\n",
      "      \"Weld\": \"Yes\",\n",
      "      \"Single Flare or Form\": \"Yes\",\n",
      "      \"Double Flare or Form\": \"Yes\",\n",
      "      \"Compression Fitting\": \"Yes\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"B\",\n",
      "      \"Braze\": \"Yes\",\n",
      "      \"Weld\": \"Yes\",\n",
      "      \"Single Flare or Form\": \"No\",\n",
      "      \"Double Flare or Form\": \"Yes\",\n",
      "      \"Compression Fitting\": \"Yes\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"BF\",\n",
      "      \"Braze\": \"Yes\",\n",
      "      \"Weld\": \"Yes\",\n",
      "      \"Single Flare or Form\": \"No\",\n",
      "      \"Double Flare or Form\": \"No (1)\",\n",
      "      \"Compression Fitting\": \"Yes\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"C\",\n",
      "      \"Braze\": \"Yes\",      \n",
      "      \"Weld\": \"Yes\",\n",
      "      \"Single Flare or Form\": \"Yes\",\n",
      "      \"Double Flare or Form\": \"Yes\",\n",
      "      \"Compression Fitting\": \"Yes\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"D\",\n",
      "      \"Braze\": \"Yes (2)\",\n",
      "      \"Weld\": \"No\",\n",
      "      \"Single Flare or Form\": \"Yes\",\n",
      "      \"Double Flare or Form\": \"Yes (3)\",\n",
      "      \"Compression Fitting\": \"Yes\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"E\",\n",
      "      \"Braze\": \"Yes\",\n",
      "      \"Weld\": \"Yes\",\n",
      "      \"Single Flare or Form\": \"Yes\",\n",
      "      \"Double Flare or Form\": \"Yes\",\n",
      "      \"Compression Fitting\": \"Yes\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"F\",\n",
      "      \"Braze\": \"Yes\",\n",
      "      \"Weld\": \"Yes\",\n",
      "      \"Single Flare or Form\": \"Yes\",\n",
      "      \"Double Flare or Form\": \"Yes\",\n",
      "      \"Compression Fitting\": \"Yes\"\n",
      "    }\n",
      "]\n",
      "\"Note\": [{\"Note (1)\": \"End may be double flared using special procedures to ensure no cracking occurs.\", \"Note (2)\": \"Grade D may be brazed using special precautions.\", \"Note (3)\": \"Double flared ends are recommended for all high-pressure applications such as brake and hydraulic service lines\"}]\n",
      "},\n",
      "\n",
      "{\"3. GRADES\":\"\n",
      "3.1 LOCAL / NATIONAL STANDARDS\n",
      "Applicable National Standards for hydraulic line tubing are listed in Table 2. In general, it is permissible to produce a part from any of these comparable National Standard grades for a corresponding CNH Grade when no special strength or composition requirements are specified. Appendix A, Table A1 lists former National Standards and Materials that have been superseded and are no longer to be used for the CNH Grades of MAT1130.\"},\n",
      "{\"4. RELATED SPECIFICATIONS\":\"\n",
      "CNH DWGA100 (86629322) Engineering Drawing Standard-Drawing Sheet Sizes and Formats\n",
      "CNH DWGA115 (87034358) Engineering Drawing Standard Notes on Drawings\n",
      "CNH DWGD110 (87034360) Recommended Practices for Hydraulic Tubing\n",
      "CNH ENPJ100 (86619032) Significant Characteristics\n",
      "CNH ENS0150 (87692757) Fabrication of Hydraulic Tubes and Ports\n",
      "CNH ENS0155 (84182620) ORFS Formed Tube Connections\n",
      "CNH ENS0160 (47646773) Fluid Conveyance Design. (Not for Supplier Distribution Confidential)\n",
      "CNH ES-H277 SSL/CTL Hydraulic Tube Requirements\n",
      "CNH MAT0310 (87303277) Zinc Plating\n",
      "CNH MAT1003 (86979049) Metallic Material Designations on Engineering Drawings\n",
      "CNH SU-216 Engineering Specification Rigid Tubes for High Pressure Application\n",
      "EN 10305 -Steel Tubes for Precision Applications – Technical Delivery Conditions\n",
      "Part 2, EN 10305-2 Welded Cold Drawn Tubes\n",
      "Part 3, EN 10305-3 Welded Cold Sized Tubes\n",
      "Part 4, EN 10305-4 Seamless Cold Drawn Tubes for Hydraulic and Pneumatic Power Systems\n",
      "Part 6, EN 10305-6 Welded and Cold Drawn Tubes for Hydraulic and Pneumatic Power Systems\n",
      "SAE J524 Seamless Low-Carbon Steel Tubing Annealed for Bending and Flaring\n",
      "SAE J525 Welded and Cold Drawn Low-Carbon Tubing Annealed for Bending and Flaring\n",
      "SAE J526 Welded Low-Carbon Steel Tubing Suitable for Bending, Flaring, Beading, Forming and\n",
      "Brazing\n",
      "SAE J527 Brazed Double Wall Low-Carbon Steel Tubing\n",
      "SAE J2614 Welded and Cold Drawn, High Strength (500 Mpa Tensile Strength) Hydraulic Tubing, for\n",
      "Bending, Flaring, Cold Forming, Welding and Brazing\n",
      "SAE J2467 Welded and Cold drawn, SAE 1021 Carbon Steel Tubing Normalized for Bending and flaring\n",
      "SAE J2551-1 Recommended Practices for Fluid Conductor Carbon, Alloy and High Strength Low Alloy, Steel Tubing Applications-Part 1: Design and Fabrication\"},\n",
      "{\n",
      "  \"Table\": \"TABLE 4: Former CNH Company Material Specifications\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"CNH Grade\": \"CNH Grade\",\n",
      "      \"CASE\": \"CASE\",\n",
      "      \"NH Pennsylvania FNHA Standard\": \"NH Pennsylvania FNHA Standard\",\n",
      "      \"NH-Zedelgem Engineering Stds.\": \"NH-Zedelgem Engineering Stds.\",\n",
      "      \"NH Tractor Engineering\": \"NH Tractor Engineering\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"A\",\n",
      "      \"CASE\": \"MS-429, Grade NF\",\n",
      "      \"NH Pennsylvania FNHA Standard\": \"2.10.1 (SAE J525)\",\n",
      "      \"NH-Zedelgem Engineering Stds.\": \"CN 63.25, St 35, SAE J525\",\n",
      "      \"NH Tractor Engineering\": \"BS6323 CEW2 NBK\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"B\",\n",
      "      \"CASE\": \"MS-429, Grade F\",\n",
      "      \"NH Pennsylvania FNHA Standard\": \"2.10.1 and 2.10.4 (1)\",\n",
      "      \"NH-Zedelgem Engineering Stds.\": \"(SAE J356)\",\n",
      "      \"NH Tractor Engineering\": \"---\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"BF\",\n",
      "      \"CASE\": \"MS-426\",\n",
      "      \"NH Pennsylvania FNHA Standard\": \"---\",\n",
      "      \"NH-Zedelgem Engineering Stds.\": \"SAE J526\",\n",
      "      \"NH Tractor Engineering\": \"---\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"C\",\n",
      "      \"CASE\": \"MS-429, Grade S\",\n",
      "      \"NH Pennsylvania FNHA Standard\": \"---\",\n",
      "      \"NH-Zedelgem Engineering Stds.\": \"CN 63.25, St 35\",\n",
      "      \"NH Tractor Engineering\": \"C12 Tubo TRF RCT BS 6323 CFS3 GBK/GZF\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"D\",\n",
      "      \"CASE\": \"MS-428\",\n",
      "      \"NH Pennsylvania FNHA Standard\": \"2.10.1 (ASTM A254)\",\n",
      "      \"NH-Zedelgem Engineering Stds.\": \"---\",\n",
      "      \"NH Tractor Engineering\": \"Fiat PS 9.02145\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"E\",\n",
      "      \"CASE\": \"---\",\n",
      "      \"NH Pennsylvania FNHA Standard\": \"---\",\n",
      "      \"NH-Zedelgem Engineering Stds.\": \"---\",\n",
      "      \"NH Tractor Engineering\": \"---\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"F\",\n",
      "      \"CASE\": \"---\",\n",
      "      \"NH Pennsylvania FNHA Standard\": \"---\",\n",
      "      \"NH-Zedelgem Engineering Stds.\": \"---\",\n",
      "      \"NH Tractor Engineering\": \"---\"\n",
      "    }\n",
      "  ],\n",
      "  \"note\": \"(1) FNHA 2.10.4 is tubing with flash in; other Grade B Former materials are flash controlled tubing.\"\n",
      "},\n",
      "\n",
      "{\"5. REQUIREMENTS\":\"\n",
      "5.1 CNH Industrial hydraulic line tubing shall meet all requirements of the applicable National Standard and any special requirements specified on the engineering drawing. It is recommended that steel hydraulic tubing be manufactured per established guidelines such as CNH ENS0150, CNH ENS0155, CNH DWGD110, or similar.\n",
      "\n",
      "5.2 DIMENSIONS AND TOLERANCES\n",
      "Hydraulic line tubing products shall conform to the permissible variations in dimensions as specified in the designated National Standard.\"},\n",
      "\n",
      "{\"6. SPECIAL REQUIREMENTS\":\"\n",
      "6.1 Optional conditions or special grades listed in National Standards may be applied as special requirements (SPCL). These requirements shall be indicated on the engineering drawing and apply to both the CNH grade and National Standard specified.\n",
      "6.2 SPECIAL GRADES\n",
      "Hydraulic line tubing fabricated from higher strength or stainless steels may be available and can be designated as a special grade, see Section 8.2.3 for an example drawing note.\n",
      "6.3 CORROSION PROTECTION\n",
      "Corrosion protection of hydraulic tubing assemblies is typically provided by paint and/or by zinc electroplating. Zinc plating may be specified per CNH MAT0310 for application prior to or after tube bending. MAT0310 provides corrosion resistance requirements for applicable plating types and classes. Another option to improve corrosion protection is to apply zinc-aluminum coatings such as Galfan (commercial coating). Note that for tubing bent after plating, corrosion resistance in the bent areas will typically be reduced compared to straight areas.\"},\n",
      "{\"7. INSPECTION AND REJECTION\":\"\n",
      "All hydraulic line tubing supplied to this specification shall be equivalent in every respect to samples approved by the purchasing CNH Industrial location. While the purchasing location may test samples from incoming shipments for quality assurance, the supplier is responsible for ensuring that shipments meet the stated requirements without depending upon the purchaser’s inspection.\"},\n",
      "{\"8. DRAWING SPECIFICATIONS\":\"\n",
      "8.1 Specifying a restricted (RSTR) or special (SPCL) material may require the use of a significant characteristic per CNH Industrial Engineering Procedure ENPJ100 (86619032). This determination will be made as part of the design review process.\n",
      "8.2 The title block of engineering drawings shall contain material designations, including product shape, as described in CNH MAT1003 (86979049) Material Designations on Engineering Drawings. The following are examples of material designations for hydraulic line tubing on drawings:\n",
      "\n",
      "8.2.1 Preferred materials; also see Table 2.\n",
      "LOCAL MATERIAL: HYD LINE TUBING, EN 10305-6, E235\n",
      "CNH MATERIAL: HYD LINE TUBING, CNH MAT1130, GRADE A\n",
      "\n",
      "8.2.2 Restricted Material (RSTR):\n",
      "LOCAL MATERIAL:SEE NOTES\n",
      "CNH MATERIAL:HYD LINE TUBING, CNH MAT1130, GRADE F, RSTR\n",
      "MATERIAL NOTE (on drawing):HYD LINE TUBING, SAE J2467, NO ALTERNATE MATERIAL PERMITTED\n",
      "8.2.3 Special Requirements (SPCL)\n",
      "LOCAL MATERIAL: HYD LINE TUBING, SAE J2833\n",
      "CNH MATERIAL: HYD LINE TUBING, CNH MAT1130, SPCL\"},\n",
      "\n",
      "{\"9. DESIGN PROPERTIES\":{\"\n",
      "9.1 Mechanical properties for comparable National Standard material grades are grouped together in Table 5 for engineering guidance.\n",
      "9.2 CNH Industrial Engineering must consider the differences in dimensional tolerances and mechanical properties of referenced National Standard hydraulic line tubing to assure functionality, reliability, and serviceability of a part when manufactured in different parts of the world. In applications where these differences cannot be compensated for in the design, the material may be restricted to the originally specified local material as described in CNH MAT1003 (86979049) Metallic Material Designations on Engineering Drawings.\"},\n",
      "\n",
      "{\n",
      "  \"Table\": \"Table 5: Mechanical Properties\",\n",
      "  \"data\": [\n",
      "    \n",
      "    {\n",
      "      \"CNH Grade\": \"A\",\n",
      "      \"Local Material / National Standard\": \"SAE J525\",\"EN 10305-6, E235\"\n",
      "      \"Tensile Strength, Mpa (psi)\": \"≥310 (≥45,000), 340-480 (49,500-69,500)\",\n",
      "      \"Yield Strength,MPa (psi)\": \"170 (24,700), 235 (34,100)\",\n",
      "      \"Min Elongation % Min.\": \"35, 25\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"B\",\n",
      "      \"Local Material / National Standard\": \"SAE J356\",\"EN 10305-3, E235+N, Option 3\"\n",
      "      \"Tensile Strength, Mpa (psi)\": \"≥310 (≥45,000), 340-480 (49,500-69,500)\",\n",
      "      \"Yield Strength,MPa (psi)\": \"170 (24,700), 235 (34,100)\",\n",
      "      \"Min Elongation % Min.\": \"35, 25\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"BF\",\n",
      "      \"Local Material / National Standard\": \"SAE J526\", \"EN 10305-3, E235+N\"\n",
      "      \"Tensile Strength, Mpa (psi)\": \"≥290 (≥42,000), 340-480 (49,500-69,500)\",\n",
      "      \"Yield Strength,MPa (psi)\": \"170 (24,700), 235 (34,100)\",\n",
      "      \"Min Elongation % Min.\": \"14, 25\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"C\",\n",
      "      \"Local Material / National Standard\": \"SAE J524\", \"EN 10305-4, E235\",\n",
      "      \"Tensile Strength, Mpa (psi)\": \"2310 (245,000), 340-480 (49,500-69,500)\",\n",
      "      \"Yield Strength,MPa (psi)\": \"170 (24,700), 235 (34,100)\",\n",
      "      \"Min Elongation % Min.\": \"35, 25\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"D\",\n",
      "      \"Local Material / National Standard\": \"SAE J527\", \"DIN 74234, Type B\", \"ISO 4038\"\n",
      "      \"Tensile Strength, Mpa (psi)\": \"≥290 (242,000)\", \"≥290 (242,000)\", \"≥290 (242,000)\"\n",
      "      \"Yield Strength,MPa (psi)\": \"170 (24,700)\", \"200 (29,000)\", \"200 (29,000)\"\n",
      "      \"Min Elongation % Min.\": \"14\",\"25\", \"25\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"E (1)\",\n",
      "      \"Local Material / National Standard\": \"SAE J2614\",\n",
      "      \"Tensile Strength, Mpa (psi)\": \"≥500 (≥72,500), 490-630 (71,000-91,500)\"\n",
      "      \"Yield Strength,MPa (psi)\": \"275 (39,900)\", \"275 (39,900)\",\n",
      "      \"Min Elongation % Min.\":\"25\", \"21\"\n",
      "    }]\n",
      "}\n",
      "},\n",
      "\n",
      "{\"Annex A\": {\"Former National Standard Materials\n",
      "A1. Table A1 lists National Standard materials that were previously listed as comparable materials for the CNH Grades. These materials might be designated on existing Company drawings and are provided here for reference only. The Standards and Grades listed in Table A1 have been superseded shall not be used on new or updated drawings.\"},\n",
      "\n",
      "{\n",
      "  \"Table\": \"TABLE A1: Former National Standard Materials (for Reference Only)\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"CNH Grade\": \"A\",\n",
      "      \"DIN\": \"DIN 2393-2,RSt 37-2, NBK\",\n",
      "      \"BS\": \"BS6323-6,CEW2 NBK\",\n",
      "      \"EN\": \"EN 10305-2, E235+N\"\n",
      "      \"FIAT\":\"---\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"B\",\n",
      "      \"DIN\": \"DIN 2394-2,RSt 37-2, NBK IG\",\n",
      "      \"BS\": \"BS 6323-5,ERW2 NBK,Flash Controlled\",\n",
      "       \"EN\": \"---\",\n",
      "       \"FIAT\": \"---\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"BF\",\n",
      "      \"DIN\": \"DIN 2394-2,RSt 37-2, NBK\",\n",
      "      \"BS\": \"BS 6323-5,ERW2 NBK\",\n",
      "       \"EN\": \"---\",\n",
      "       \"FIAT\": \"---\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"C\",\n",
      "      \"DIN\": \"DIN 2394-2,St 35, NBK\",\n",
      "      \"BS\": \"BS 6323-4,CFS3 GBK\",\n",
      "       \"EN\": \"---\",\n",
      "       \"FIAT\": \"Fiat 52808,C12 Tubo TRF RCT\"\n",
      "    },\n",
      "    {\n",
      "      \"CNH Grade\": \"D\",\n",
      "      \"DIN\": \"---\",\n",
      "      \"BS\": \"---\",\n",
      "       \"EN\": \"---\",\n",
      "       \"FIAT\": \"Fiat PS 9.02145\"\n",
      "    },\n",
      "   {\n",
      "      \"CNH Grade\": \"C\",\n",
      "      \"DIN\": \"DIN 2394-2,St 35, NBK\",\n",
      "      \"BS\": \"BS 6323-4,CFS5 NBK\",\n",
      "       \"EN\": \"---\",\n",
      "       \"FIAT\": \"---\"\n",
      "    }\n",
      "]\n",
      "\"Note 1\": (1) Referenced DIN and BS specifications have been superseded by European Standard EN 10305\n",
      "}\n",
      "}\n",
      "    \n",
      "\n",
      "'''assistant\n",
      "\n",
      "Here is a 1200-word summary of the text:\n",
      "\n",
      "**Material Specification for Hydraulic Line Tubing - MAT1130**\n",
      "\n",
      "The Material Specification for Hydraulic Line Tubing - MAT1130 is a document that provides reference information for CNH hydraulic line tubing grades listed in Table 1. It is intended to replace former CNH Company Material Specifications and should be used on all applicable new and updated engineering drawings.\n",
      "\n",
      "**CNH Material Grades and Descriptions**\n",
      "\n",
      "The document lists six CNH material grades and their corresponding descriptions:\n",
      "\n",
      "1. CNH Grade A: Cold Drawn, Single Wall, Welded Tubing, with No Internal Flash. This grade is suitable for applications involving transmission of fluid at higher pressure and is recommended where high rates of vibration or surge loading are experienced.\n",
      "2. CNH Grade B: Single Wall, Welded Tubing with Flash Control (0.25 mm / 0.010\" max. flash height). This grade is not recommended for applications involving high rates of vibration or surge loading unless testing verifies acceptability.\n",
      "3. CNH Grade BF: Single Wall Welded Tubing with Flash. This grade is not recommended for applications involving high rates of vibration or surge loading unless testing verifies acceptability. This tubing is not easily formed and requires a larger bend radius than Grades A or B.\n",
      "4. CNH Grade C: Cold Drawn, Single Wall Seamless Tubing. This grade is suitable for applications involving transmission of fluid at higher pressure and has more liberal dimensional tolerances than Grades A or B.\n",
      "5. CNH Grade D: Brazed Double Wall Tubing (commercially known as Bundy Tube). This grade is used for high-pressure, critical applications such as brake and hydraulic service lines.\n",
      "6. CNH Grade E: Cold Drawn, Single Wall Seamless Tubing (EN 10305-4, E355) or Welded Tubing with no Internal Flash (SAE J2614). This grade is used in high-pressure systems such as 40 MPa (400 bar, 5800 psi) excavator hydraulic systems.\n",
      "7. CNH Grade F: Cold\n"
     ]
    }
   ],
   "source": [
    "# print the output\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE INPUT SUMMARY USING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf30b37d825d42a485596dc51f33513f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817d16e0ab8d45e5a6b28a26b950fd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b343f6dabd5643e6ae16730e114a1a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "llama3_hf_token = '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'\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token)\n",
    "# create pipeline with cuda\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", token = llama3_hf_token, device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"/home/vp899/projects/llm_pilot/Data/MAT1130_Context.txt\"\n",
    "# read the contents of text_file into str\n",
    "with open(text_file, 'r') as file:\n",
    "    input_text = file.read()\n",
    "# print(text)\n",
    "system_prompt = \"You are a helpful digital assistant. You will provide clear and factual answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Can you please create a 1200 word summary of the text within the triple quotes?. Include all the quantitative details ''' \" + input_text + \"'''\"},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model_output = pipe(messages, max_new_tokens=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Here is a 1200-word summary of the text:\\n\\nThe Material Specification for Hydraulic Line Tubing (MAT1130) provides reference information for CNH hydraulic line tubing grades listed in Table 1. This specification is intended to replace former CNH Company Material Specifications and should be used on all applicable new and updated engineering drawings.\\n\\nThe specification outlines the different CNH Material Grades and their descriptions, including Grade A, Grade B, Grade BF, Grade C, Grade D, Grade E, and Grade F. Each grade has its own unique characteristics, such as cold drawn, single wall, welded tubing, and flash control. The grades are categorized based on their material properties, such as tensile strength, yield strength, and elongation.\\n\\nTable 2 lists the local material and national standard references for each CNH Material Grade. The table shows the corresponding national standards and materials that can be used as alternatives to the CNH Material Grades.\\n\\nTable 3 provides a guide for tube end connector applications for each CNH Material Grade. The table shows whether the grade can be brazed, welded, or used with single or double flares.\\n\\nThe specification also outlines the requirements for hydraulic line tubing, including dimensions and tolerances, mechanical properties, and corrosion protection. The tubing must conform to the permissible variations in dimensions as specified in the designated national standard.\\n\\nThe specification also includes special requirements, such as restricted or special materials, and provides examples of material designations for hydraulic line tubing on drawings.\\n\\nTable 5 provides mechanical properties for each CNH Material Grade, including tensile strength, yield strength, and elongation.\\n\\nThe specification also includes an annex that lists former national standard materials that were previously listed as comparable materials for the CNH Grades. These materials are provided for reference only and should not be used on new or updated drawings.\\n\\nIn summary, the Material Specification for Hydraulic Line Tubing (MAT1130) provides a comprehensive guide for the design, manufacture, and application of hydraulic line tubing. The specification outlines the different CNH Material Grades and their characteristics, provides guidelines for tube end connector applications, and outlines the requirements for hydraulic line tubing, including dimensions and tolerances, mechanical properties, and corrosion protection.\\n\\nHere are some key quantitative details:\\n\\n* Tensile strength: 310-480 MPa (45,000-69,500 psi) for Grade A, 290-480 MPa (42,000-69,500 psi) for Grade BF, and 500 MPa (72,500 psi) for Grade E.\\n* Yield strength: 170-235 MPa (24,700-34,100 psi) for Grade A, 170-235 MPa (24,700-34,100 psi) for Grade B, and 275 MPa (39,900 psi) for Grade E.\\n* Elongation: 35% minimum for Grade A, 25% minimum for Grade B, and 21% minimum for Grade E.\\n* Dimensions and tolerances: The tubing must conform to the permissible variations in dimensions as specified in the designated national standard.\\n* Corrosion protection: The tubing may be coated with zinc or zinc-aluminum to improve corrosion resistance.\\n\\nNote that these quantitative details are specific to the CNH Material Grades listed in the specification and may not be applicable to other materials or grades.'}\n"
     ]
    }
   ],
   "source": [
    "print(model_output[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV cache experiment\n",
    "## Parse output before sending to decoder [based on example in HF model card]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e69c33294ad4d96aa20a793a5c73fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", torch_dtype = torch.float16) ## pip install accelerate based on error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generation_config.cache_implementation = \"static\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"/home/vp899/projects/llm_pilot/Data/MAT1130_Context.txt\"\n",
    "# read the contents of text_file into str\n",
    "with open(text_file, 'r') as file:\n",
    "    input_text = file.read()\n",
    "# print(text)\n",
    "system_prompt = \"You are a helpful digital assistant. You will provide clear and factual answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496\n"
     ]
    }
   ],
   "source": [
    "# find the number of words in the input_text\n",
    "num_words = len(input_text.split())\n",
    "# print the number of words\n",
    "print(num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Can you please create a 1200 word summary of the text within the triple quotes?. Include all the quantitative details ''' \" + input_text + \"'''\"},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiled_model = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "# compiled_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes to tokenizer input based on HF model card example -- 5/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:140: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: Triton Error [CUDA]: device kernel image is invalid\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model_output = model.generate(model_inputs, max_length = 5500, pad_token_id=tokenizer.eos_token_id, do_sample=True, temperature=0.8, top_p =0.9)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      3\u001b[0m     model_inputs,\n\u001b[1;32m      4\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      5\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mterminators,\n\u001b[1;32m      6\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[1;32m      8\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m      9\u001b[0m     cache_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   1623\u001b[0m         input_ids,\n\u001b[1;32m   1624\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1625\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[1;32m   1626\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1627\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1628\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1629\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[1;32m   1630\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1631\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1632\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1633\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1634\u001b[0m     )\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/transformers/generation/utils.py:2791\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2793\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2794\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2795\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2796\u001b[0m )\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2799\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callback(frame, cache_entry, hooks, frame_state)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:383\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state)\u001b[0m\n\u001b[1;32m    370\u001b[0m signpost_event(\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     },\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpatch(_patch_config_if_changed()):\n\u001b[0;32m--> 383\u001b[0m     compiled_product \u001b[38;5;241m=\u001b[39m _compile(\n\u001b[1;32m    384\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_code,\n\u001b[1;32m    385\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_globals,\n\u001b[1;32m    386\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_locals,\n\u001b[1;32m    387\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_builtins,\n\u001b[1;32m    388\u001b[0m         compiler_fn,\n\u001b[1;32m    389\u001b[0m         one_graph,\n\u001b[1;32m    390\u001b[0m         export,\n\u001b[1;32m    391\u001b[0m         export_constraints,\n\u001b[1;32m    392\u001b[0m         hooks,\n\u001b[1;32m    393\u001b[0m         cache_size,\n\u001b[1;32m    394\u001b[0m         frame,\n\u001b[1;32m    395\u001b[0m         frame_state\u001b[38;5;241m=\u001b[39mframe_state,\n\u001b[1;32m    396\u001b[0m         compile_id\u001b[38;5;241m=\u001b[39mcompile_id,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_product\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:646\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 646\u001b[0m         guarded_code \u001b[38;5;241m=\u001b[39m compile_inner(code, one_graph, hooks, transform)\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    649\u001b[0m         Unsupported,\n\u001b[1;32m    650\u001b[0m         TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m         BisectValidationException,\n\u001b[1;32m    658\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/utils.py:244\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    243\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 244\u001b[0m     r \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    245\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    246\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:562\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    560\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1033\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1030\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1031\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1033\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:151\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:527\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 527\u001b[0m         tracer\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    529\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2128\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2128\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:818\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 818\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    819\u001b[0m     ):\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:781\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    778\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    780\u001b[0m     )\n\u001b[0;32m--> 781\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)(inst)\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2243\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2238\u001b[0m _step_logger()(\n\u001b[1;32m   2239\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   2240\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2241\u001b[0m )\n\u001b[1;32m   2242\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mcompile_subgraph(\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2245\u001b[0m     reason\u001b[38;5;241m=\u001b[39mGraphCompileReason(\n\u001b[1;32m   2246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_summary()], graph_break\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m     ),\n\u001b[1;32m   2248\u001b[0m     compile_return_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2249\u001b[0m )\n\u001b[1;32m   2250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:945\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason, compile_return_value)\u001b[0m\n\u001b[1;32m    942\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_calls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    944\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 945\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_and_call_fx_graph(tx, pass2\u001b[38;5;241m.\u001b[39mgraph_output_vars(), root)\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    949\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(pass2\u001b[38;5;241m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1087\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1087\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_user_compiler(gm)\n\u001b[1;32m   1088\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m   1090\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/utils.py:244\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    243\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 244\u001b[0m     r \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    245\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    246\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1159\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1160\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1161\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m signpost_event(\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     },\n\u001b[1;32m   1172\u001b[0m )\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1140\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1139\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1140\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m compiler_fn(gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_inputs())\n\u001b[1;32m   1141\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/__init__.py:1668\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:952\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_patches:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpatch(config_patches):\n\u001b[0;32m--> 952\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m compile_fx(\n\u001b[1;32m    953\u001b[0m             model_,\n\u001b[1;32m    954\u001b[0m             example_inputs_,\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;66;03m# need extra layer of patching as backwards is compiled out of scope\u001b[39;00m\n\u001b[1;32m    956\u001b[0m             inner_compile\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpatch(config_patches)(inner_compile),\n\u001b[1;32m    957\u001b[0m             decompositions\u001b[38;5;241m=\u001b[39mdecompositions,\n\u001b[1;32m    958\u001b[0m         )\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcpp_wrapper:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[1;32m    962\u001b[0m         {\n\u001b[1;32m    963\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpp_wrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         }\n\u001b[1;32m    968\u001b[0m     ), V\u001b[38;5;241m.\u001b[39mset_real_inputs(example_inputs_):\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1168\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[1;32m   1166\u001b[0m     tracing_context\n\u001b[1;32m   1167\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m-> 1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m   1169\u001b[0m         fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[1;32m   1170\u001b[0m         bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[1;32m   1171\u001b[0m         inference_compiler\u001b[38;5;241m=\u001b[39minference_compiler,\n\u001b[1;32m   1172\u001b[0m         decompositions\u001b[38;5;241m=\u001b[39mdecompositions,\n\u001b[1;32m   1173\u001b[0m         partition_fn\u001b[38;5;241m=\u001b[39mpartition_fn,\n\u001b[1;32m   1174\u001b[0m         keep_inference_input_mutations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1175\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:55\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 55\u001b[0m         cg \u001b[38;5;241m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     56\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:887\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[1;32m    871\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[1;32m    872\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[1;32m    873\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    883\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    884\u001b[0m )\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m--> 887\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m    888\u001b[0m         functional_call,\n\u001b[1;32m    889\u001b[0m         full_args,\n\u001b[1;32m    890\u001b[0m         aot_config,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;66;03m# convention.  This should get fixed...\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39mruntime_args):\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/utils.py:244\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    243\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 244\u001b[0m     r \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    245\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    246\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:600\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m    597\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m partial(aot_wrapper_dedupe, compiler_fn\u001b[38;5;241m=\u001b[39mcompiler_fn)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# You can put more passes here\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata\u001b[38;5;241m=\u001b[39mfw_metadata)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mis_export:\n\u001b[1;32m    602\u001b[0m     mutated_user_inp_locs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    603\u001b[0m         idx \u001b[38;5;241m-\u001b[39m aot_config\u001b[38;5;241m.\u001b[39mnum_params_buffers\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m fw_metadata\u001b[38;5;241m.\u001b[39mmutated_inp_runtime_indices\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m aot_config\u001b[38;5;241m.\u001b[39mnum_params_buffers\n\u001b[1;32m    606\u001b[0m     ]\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:425\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[0;32m--> 425\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata\u001b[38;5;241m=\u001b[39mfw_metadata)\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(leaf_flat_args, fw_metadata):\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03mEncountered duplicate inputs that are mutated in the graph, but at least one input/output\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03mto the graph is a tensor subclass. This is not supported today. You can try to\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03mremove the aliasing yourself as a workaround, or otherwise file an issue on github.\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m         )\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:630\u001b[0m, in \u001b[0;36maot_wrapper_synthetic_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m# Happy path: we don't need synthetic bases\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synthetic_base_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiler_fn(flat_fn, flat_args, aot_config, fw_metadata\u001b[38;5;241m=\u001b[39mfw_metadata)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# export path: ban synthetic bases for now, add later if requested.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(flat_args, fw_metadata):\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:97\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tracing_context \u001b[38;5;241m:=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mTracingContext\u001b[38;5;241m.\u001b[39mtry_get():\n\u001b[1;32m     92\u001b[0m         tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m             fw_metadata\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m maybe_subclass_meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m maybe_subclass_meta\u001b[38;5;241m.\u001b[39mfw_metadata\n\u001b[1;32m     96\u001b[0m         )\n\u001b[0;32m---> 97\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m compiler(fw_module, updated_flat_args)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# This boxed_call handling happens inside create_runtime_wrapper as well.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# However, create_runtime_wrapper does not expect the rng offsets in the\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# output. So, we have to create another wrapper and take out the offset. As\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# a result, we have to account for not boxed_call compilers as well.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/utils.py:244\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    243\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 244\u001b[0m     r \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    245\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    246\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1100\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m orig_output_end_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_model_outputs\n\u001b[1;32m   1094\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1095\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[original_output_start_index:orig_output_end_idx]\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[1;32m   1098\u001b[0m     }\n\u001b[0;32m-> 1100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_compile(\n\u001b[1;32m   1101\u001b[0m     model,\n\u001b[1;32m   1102\u001b[0m     example_inputs,\n\u001b[1;32m   1103\u001b[0m     num_fixed\u001b[38;5;241m=\u001b[39mfixed,\n\u001b[1;32m   1104\u001b[0m     cudagraphs\u001b[38;5;241m=\u001b[39mcudagraphs,\n\u001b[1;32m   1105\u001b[0m     graph_id\u001b[38;5;241m=\u001b[39mgraph_id,\n\u001b[1;32m   1106\u001b[0m     is_inference\u001b[38;5;241m=\u001b[39mis_inference,\n\u001b[1;32m   1107\u001b[0m     boxed_forward_device_index\u001b[38;5;241m=\u001b[39mforward_device,\n\u001b[1;32m   1108\u001b[0m     user_visible_outputs\u001b[38;5;241m=\u001b[39muser_visible_outputs,\n\u001b[1;32m   1109\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py:83\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/debug.py:305\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:320\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    316\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m FxGraphCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    317\u001b[0m         fx_codegen_and_compile, gm, example_inputs, graph_kwargs\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m fx_codegen_and_compile(\n\u001b[1;32m    321\u001b[0m         gm, example_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgraph_kwargs  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX codegen and compilation took \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# Return the output strides to the caller via TracingContext\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:550\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m             output_strides\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 550\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcompile_to_fn()\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m V\u001b[38;5;241m.\u001b[39maot_compilation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/graph.py:1116\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AotCodeCache\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28mself\u001b[39m, code, serialized_extern_kernel_nodes, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda\n\u001b[1;32m   1114\u001b[0m     )\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_to_module()\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_dynamo/utils.py:244\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    243\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 244\u001b[0m     r \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    245\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    246\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/graph.py:1070\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1068\u001b[0m linemap \u001b[38;5;241m=\u001b[39m [(line_no, node\u001b[38;5;241m.\u001b[39mstack_trace) \u001b[38;5;28;01mfor\u001b[39;00m line_no, node \u001b[38;5;129;01min\u001b[39;00m linemap]\n\u001b[1;32m   1069\u001b[0m key, path \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mwrite(code)\n\u001b[0;32m-> 1070\u001b[0m mod \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mload_by_key_path(\n\u001b[1;32m   1071\u001b[0m     key, path, linemap\u001b[38;5;241m=\u001b[39mlinemap, attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants\n\u001b[1;32m   1072\u001b[0m )\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_key \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_path \u001b[38;5;241m=\u001b[39m path\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/codecache.py:1892\u001b[0m, in \u001b[0;36mPyCodeCache.load_by_key_path\u001b[0;34m(cls, key, path, linemap, attrs)\u001b[0m\n\u001b[1;32m   1890\u001b[0m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m   1891\u001b[0m mod\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1892\u001b[0m exec(code, mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n\u001b[1;32m   1893\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m mod\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;66;03m# another thread might set this first\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/torchinductor_vp899/ni/cniz6erw67wek5364pboien62n35tettcpzpougzqftsnikfv2c3.py:1327\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# kernel path: /tmp/torchinductor_vp899/ef/cef5kw57ltf4wbamztvgy3ayuye5iujqbyvwpffvbgo6bpqipc7k.py\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# Source Nodes: [logits_1], Original ATen: [aten._to_copy]\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# logits_1 => convert_element_type_740\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m triton_poi_fused__to_copy_21 \u001b[38;5;241m=\u001b[39m async_compile\u001b[38;5;241m.\u001b[39mtriton(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriton_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124mimport triton\u001b[39m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124mimport triton.language as tl\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;124m    tl.store(out_ptr0 + (x0), tmp1, xmask)\u001b[39m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n\u001b[0;32m-> 1327\u001b[0m async_compile\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m async_compile\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(args):\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/codecache.py:2486\u001b[0m, in \u001b[0;36mAsyncCompile.wait\u001b[0;34m(self, scope)\u001b[0m\n\u001b[1;32m   2484\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(key)\n\u001b[1;32m   2485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, (Future, TritonFuture)):\n\u001b[0;32m-> 2486\u001b[0m             scope[key] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m   2487\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2489\u001b[0m _compile_end()\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/codecache.py:2330\u001b[0m, in \u001b[0;36mTritonFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;66;03m# If the worker failed this will throw an exception.\u001b[39;00m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m-> 2330\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m _load_kernel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_code)\n\u001b[1;32m   2331\u001b[0m latency \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m   2332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latency \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m50\u001b[39m:\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/codecache.py:2306\u001b[0m, in \u001b[0;36m_load_kernel\u001b[0;34m(kernel_name, source_code)\u001b[0m\n\u001b[1;32m   2304\u001b[0m _set_triton_ptxas_path()\n\u001b[1;32m   2305\u001b[0m kernel \u001b[38;5;241m=\u001b[39m TritonCodeCache\u001b[38;5;241m.\u001b[39mload(kernel_name, source_code)\n\u001b[0;32m-> 2306\u001b[0m kernel\u001b[38;5;241m.\u001b[39mprecompile()\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kernel\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py:188\u001b[0m, in \u001b[0;36mCachingAutotuner.precompile\u001b[0;34m(self, warm_cache_only_with_cc)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m         compiled_binary, launcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precompile_config(\n\u001b[1;32m    189\u001b[0m             c, warm_cache_only_with_cc\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m OutOfResources:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# Skip the config if we run out of resource\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py:308\u001b[0m, in \u001b[0;36mCachingAutotuner._precompile_config\u001b[0;34m(self, cfg, warm_cache_only_with_cc)\u001b[0m\n\u001b[1;32m    303\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device())\n\u001b[1;32m    304\u001b[0m     binary \u001b[38;5;241m=\u001b[39m triton\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompile_meta,\n\u001b[1;32m    307\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m     binary\u001b[38;5;241m.\u001b[39m_init_handles()\n\u001b[1;32m    310\u001b[0m call_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     arg\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39marg_names)\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39mconstexprs\n\u001b[1;32m    314\u001b[0m ]\n\u001b[1;32m    315\u001b[0m def_args \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39marg_names \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mkwargs]\n",
      "File \u001b[0;32m/anaconda/envs/pi1_py311/lib/python3.11/site-packages/triton/compiler/compiler.py:683\u001b[0m, in \u001b[0;36mCompiledKernel._init_handles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared \u001b[38;5;241m>\u001b[39m max_shared:\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutOfResources(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared, max_shared, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared memory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 683\u001b[0m mod, func, n_regs, n_spills \u001b[38;5;241m=\u001b[39m fn_load_binary(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masm[bin_path], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared, device)\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_spills \u001b[38;5;241m=\u001b[39m n_spills\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_regs \u001b[38;5;241m=\u001b[39m n_regs\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nRuntimeError: Triton Error [CUDA]: device kernel image is invalid\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "# model_output = model.generate(model_inputs, max_length = 5500, pad_token_id=tokenizer.eos_token_id, do_sample=True, temperature=0.8, top_p =0.9)\n",
    "model_output = model.generate(\n",
    "    model_inputs,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    cache_implementation=\"static\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m0\u001b[39m][model_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(response, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_output' is not defined"
     ]
    }
   ],
   "source": [
    "response = model_output[0][model_inputs.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
